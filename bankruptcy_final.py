# -*- coding: utf-8 -*-
"""Bankruptcy - Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lDa5GdajvstBs-G2zg0YSSlkNL9bZU0W
"""



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import klib
from imblearn.over_sampling import BorderlineSMOTE, RandomOverSampler, SMOTE
from collections import Counter
from sklearn.pipeline import Pipeline
import warnings, gc, joblib, re
from feature_engine.selection import DropConstantFeatures, DropCorrelatedFeatures, DropDuplicateFeatures  # Fixed the typo here
warnings.filterwarnings('ignore')
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import kagglehub

#Load the dataset
path = kagglehub.dataset_download("fedesoriano/company-bankruptcy-prediction")
df_1 = pd.read_csv(path+'/data.csv')
df_1.head()

#Define features and Target
X = df_1.drop('Bankrupt?', axis=1) #features
y = df_1['Bankrupt?'] #target

#Sanity Check of Data

df_1.info() #pre , data information
#6819 instances in dataset, 2 binary

#Print Summary Info
print("\nData types:\n", df_1.dtypes.value_counts())
###summary of missing values and data types"""

#Check for Data Quality Issues
df_1.isnull().sum()        # Check for missing values
df_1.duplicated().sum()    # Check for duplicate rows
df_1.drop(df_1.columns[df_1.nunique() == 1][0], axis=1, inplace=True)  # Drop constant column
#removes columns from dataframe(df_1) where all values are identical

# Check for missing values
print("Missing values per column:", df_1.isnull().sum()) #no missing value

# Check for duplicate rows
print("Number of duplicate rows:", df_1.duplicated().sum()) #no duplicate value

# Drop column(s) with only one unique value
#print("Dropped column(s) with only one unique value.")

# Identify columns with only one unique value
cols_to_drop = df_1.columns[df_1.nunique() == 1]

# Drop and print the column names
if not cols_to_drop.empty:
    df_1.drop(columns=cols_to_drop, inplace=True)
    print("Dropped column(s) with only one unique value:", cols_to_drop.tolist())
else:
    print("No columns with only one unique value found.")
#no columns with only one unique value

#Identify Variable Types (Extra)
cat_vars = [var for var in df_1.columns if len(df_1[var].unique()) < 20]
num_vars = [var for var in df_1.columns if var not in cat_vars]
###categorical or numerical data"""
cat_vars

#iterate over all column names in dataframe , returns array of unique values in column var
#check if the column has less than 20 unique values

# 2.Exploratory Data Analysis (EDA)

#Visualize all features
df_1.hist(figsize=(50,50))
###create histogram for all columns to understand distribution"""

#detail kutosis , skew
print(df_1[num_vars].agg(['skew', 'kurtosis']).T)
klib.dist_plot(df_1[num_vars])

#filter only numerical columns , agg = computes 2 statistical measures
#skewness > 0 righted-tailed (positive skew)
#kurtosis >3 heavy tails (more outlier)

# skew with asending order(Extra)

skewness = df_1.drop(columns=['Bankrupt?']).skew().sort_values(ascending=False)
print(skewness)

#have both right tailed, left tailed, need to modify

#Outlier Detection

numeric_cols = df_1.select_dtypes(include=["float64"]).columns
for col in numeric_cols[1:10]:  # First 5 numeric columns
    plt.figure(figsize=(8, 2))
    sns.boxplot(x=df_1[col])
    plt.title(f"Boxplot of {col} after IQR capping")
    plt.show()
numeric_cols.shape

##Data Transformation

from sklearn.preprocessing import PowerTransformer

def auto_transform(df, exclude=['target']):
    df_new = df.copy()
    for col in df.columns:
        if col in exclude or not np.issubdtype(df[col].dtype, np.number): #exclude non-numerical col
            continue
        skew = df[col].skew()
        if skew > 1 and (df[col] > 0).all(): #highly right-skewed, use log1p
            df_new[col] = np.log1p(df[col])#postive value
        elif 0.5 < skew <= 1 and (df[col] >= 0).all(): #moderately skewed, square root
            df_new[col] = np.sqrt(df[col]) #non-neg
        elif abs(skew) > 0.5: # absolute value of skew >0.5 , moderate to high skew,Yeo-Johnson Power
            pt = PowerTransformer(method='yeo-johnson') # work for negative/zero values
            df_new[col] = pt.fit_transform(df[[col]])
    return df_new

df_transformed = auto_transform(df_1, exclude=['Bankrupt?'])

df_transformed.hist(figsize=(50,50))

#automatically applies transformation to numerical columns,to reduced skewness and make the data
#more normally distributed

#log1p is used instead of log to avoid errors with zeros (log(1 + x))
#compress large values and expand small ones , best for right skewed
#not for non-positive data , fails for negative values and weak for heavy-tailed data
#convert multiplicative into additive one. eg. 10*1000 = 10000, log(10)+log(1000) 4 (10^4 recover to original sacle) => 2.718 not 10


#square root is less agressive than log.
#retain more original scale. not suitable for extremely right skewed

#yeo-johnson , handle negative and 0 values
#based on input value x and power parameter lamda. the optimal lamda chosen to maximiza normality via MLE
#0 => log , 0.5 => sqrt , -1 => inverse transform(1/x)

#removing outliers

def remove_outliers_std(data, std_threshold=1.8, exclude_columns=None):
    """
    Remove outliers from a DataFrame using standard deviation threshold,
    but skip any columns listed in `exclude_columns`.

    Parameters:
    - data: pd.DataFrame
    - std_threshold: float
    - exclude_columns: list of column names to skip (e.g., target columns like 'Bankrupt?')

    Returns:
    - DataFrame with outliers replaced by NaN (except in excluded columns)
    """
    if exclude_columns is None: #initialize as empty list if not provided
        exclude_columns = []

    cleaned = data.copy() #reates a copy to avoid modifying the original DataFrame
    for col in cleaned.select_dtypes(include=[np.number]).columns: #Iterates through only numerical columns (int/float)
        if col in exclude_columns: #Skips processing for excluded columns
            continue
        mean = cleaned[col].mean() #mean and standard deviation for each column
        std = cleaned[col].std()
        cleaned[col] = cleaned[col].where(abs(cleaned[col] - mean) <= std_threshold * std)
    return cleaned #Replaces values outside mean ± (threshold × std) with NaN

# Step 1: Remove outliers from features only (exclude 'Bankrupt?')
cleaned_df = remove_outliers_std(df_transformed, std_threshold=1.8, exclude_columns=['Bankrupt?'])

# Step 2: Drop rows that had any outliers
cleaned_df = cleaned_df.dropna()

# Step 3: Extract target
y_original = cleaned_df['Bankrupt?']

#std deviation threshold = 1.8 but normal zscore is 3 std dev
#afraid of being too agressive and permissive so we use std dev as tuning parameter



cleaned_df.hist(figsize=(50,50))

cleaned_df.shape

#Standardization using z-score with +- 3 stad deviation

from sklearn.preprocessing import StandardScaler

# Separate features and target first
X_raw = cleaned_df.drop(columns=['Bankrupt?'])
y_c = cleaned_df['Bankrupt?']



scaler=StandardScaler().fit(X_raw) #compute and stores mean and std for each feature
print(scaler)
#standardize the data
data_scaled=scaler.transform(X_raw)
print(data_scaled)
print(data_scaled.mean(axis=0))#check the mean, for a standarzide data it's = 0 or near,
#check the std, for a standarzide data it's = 1,
print(data_scaled.std(axis=0)) #std dev


X_c = pd.DataFrame(data_scaled, columns=X_raw.columns, index=X_raw.index)

# Combine standardized features and original target
data_scaled_df = pd.concat([X_c, y_c], axis=1)

#approximately to mean 0 and std 1
#Liability asset flag = 0 coz imbalanced

data_scaled_df.hist(figsize=(50,50))

#Visualize Feature Correlations Using Heatmap

# Prepare feature list excluding the target variable
features = list(data_scaled_df.columns)
features.remove('Bankrupt?')

# Helper function to format column names for better display(text formatting)
import textwrap
def str_to_readable_title(s, max_lines=1):
    s = s.replace('_', ' ').capitalize().rstrip('?') + ('?' if s.endswith('?') else '')
    if max_lines > 1:
        return textwrap.fill(s, width=len(s)//max_lines)
    return s

# Compute correlation matrix and plot heatmap
sns.set(font_scale=0.7)
df_1_X_c = data_scaled_df[features[:15]]  # Take first 15 features for heatmap
df_1_c_corr = df_1_X_c.corr()
df_1_c_corr.columns = list(map(lambda colname: str_to_readable_title(colname, max_lines=3), df_1_c_corr.columns))
df_1_c_corr.index = df_1_c_corr.columns

plt.figure(figsize=(10, 8))
plt.title("Correlation Heatmap (15 Features Before Reduction)", fontsize=12)
sns.heatmap(df_1_c_corr, annot=True)
plt.show()

#Plot target Variable

plt.figure(figsize=(6,4))
plt.hist(data_scaled_df['Bankrupt?'], bins=2)
plt.xticks([0, 1])
plt.xlabel('Bankrupt?')
plt.ylabel('Count')
plt.title('Distribution of Bankrupt Companies')
plt.grid(False)
plt.show()



# Apply PCA to reduce to 2 components (For Visualization)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_c)
#Returns NumPy array with shape (n_samples, 2)

# Create a new DataFrame with PCA results and target
pca_df_1 = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
pca_df_1['Bankrupt?'] = y_c.values


# Plot using seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=pca_df_1,
    x='PC1',
    y='PC2',
    hue='Bankrupt?',
    palette={0: 'lightblue', 1: 'salmon'},
    alpha=0.6,
    edgecolor=None,

)
plt.title('PCA Projection (Before Preprocessing)')
plt.grid(False)
plt.show()

#Linear dimensionality reduction technique
#Finds orthogonal directions of maximum variance
#First PC (PC1) captures most variance, second PC (PC2) captures next most, etc

#Split Train and Test

#Split into train and test sets (train 70,test 30)
X_train, X_test, y_train, y_test = train_test_split(X_c, y_c, test_size=0.3, random_state=42) #reproducible splits

#Class distribution summary
tr = np.unique(y_train, return_counts=True)[1] #count y train
tst = np.unique(y_test, return_counts=True)[1] #count x train


# Print dataset summary
print(f"\tTrain set\n\nbankrupt instances: {tr[1]},\nnon-bankrupt instances: {tr[0]}\n")
print(f"\tTest set\n\nbankrupt instances: {tst[1]},\nnon-bankrupt instances: {tst[0]}")

#Upsampling Data

#Apply SMOTE Only to Training Data ---

smote = BorderlineSMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
print("Final dimensions of target label classes", Counter(y_train_resampled))

#focuses on samples near the decision boundary,Creates new samples via linear interpolation between neighbors
#for minority instance x1 , fina k-nearest neighbors xz1 in minority class
#generate new sample , x1+lamda(xz1-x1) , lamda~U(0,1)

#Visualize Class Distribution After SMOTE
# Create DataFrame for visualization
resampled_df_1 = pd.DataFrame(X_train_resampled, columns=X_train.columns)
resampled_df_1['Bankrupt?'] = y_train_resampled

# Plot balanced class distribution
plt.figure(figsize=(8, 5))
ax = sns.countplot(data=resampled_df_1, x='Bankrupt?', color='#4c72b0')

# Annotate bar values
for p in ax.patches:
    height = int(p.get_height())
    ax.annotate(f'{height:,}',
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom',
                fontsize=11)

# Set labels and style
ax.set_title('Distribution of Bankrupt Companies (After SMOTE)', fontsize=14, weight='semibold', pad=20)
ax.set_xlabel('Bankrupt?', fontsize=12)
ax.set_ylabel('Count', fontsize=12)
ax.set_xticklabels(['No (0)', 'Yes (1)'], fontsize=11)  # More explicit labels
ax.tick_params(axis='y', labelsize=10)

# Add informative text about resampling
plt.text(0.5, -0.15,
         f"Original train counts: {Counter(y_train)}\nSMOTE applied to training data only",
         ha='center', va='center', transform=ax.transAxes,
         bbox=dict(facecolor='white', alpha=0.8), fontsize=10)

sns.despine(trim=True)
plt.tight_layout()
plt.show()

#Visualize after upsampling using PCA

#Apply PCA (fit on train only) ---
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_resampled)
X_test_pca = pca.transform(X_test)

#Create PCA DataFrames ---
train_pca_df_1 = pd.DataFrame(data=X_train_pca, columns=['PC1', 'PC2'])
train_pca_df_1['Bankrupt?'] = y_train_resampled.values

test_pca_df_1 = pd.DataFrame(data=X_test_pca, columns=['PC1', 'PC2'])
test_pca_df_1['Bankrupt?'] = y_test.values  # assuming y_test exists, not up sampling

#Plot ---
plt.figure(figsize=(10, 6))

# Plot training data
sns.scatterplot(
    data=train_pca_df_1,
    x='PC1',
    y='PC2',
    hue='Bankrupt?',
    palette={0: 'lightblue', 1: 'salmon'},
    alpha=0.6,
    edgecolor=None
)
plt.title('PCA Projection After Preprocessing(Training Data)')
plt.grid(False)
plt.show()

#Save Train/Test Data for Further Preprocessing

df_1_train = pd.DataFrame(X_train_resampled, columns=X_c.columns[:-1])
df_1_train['bankrupt'] = y_train_resampled

df_1_test = pd.DataFrame(X_test, columns=X_c.columns[:-1]) #slice assumes target was last column in X_c
df_1_test['bankrupt'] = y_test

#Apply PCA for Feature Selection

pca = PCA()
pca.fit(X_train_resampled)

#Get absolute loadings (contributions of each feature)
loadings = np.abs(pca.components_)  # shape: (n_components, n_features)
#how much each feature contributes to each principal component.


#Score features by total contribution to top k components
k_components = 5 #total contribution to the top k_components (here, 5)
feature_scores = loadings[:k_components].sum(axis=0) #Summing across the first 5 components
#Scores each feature by total contribution to the top k_components (here, 5).

#Automatically drop features below threshold (e.g., mean contribution)
threshold = np.mean(feature_scores)
keep_mask = feature_scores >= threshold
selected_indices = np.where(keep_mask)[0]
#Filters features whose contribution is at least as large as the mean.
#Features contributing more than average are considered informative,
#while others are discarded.


# Use original feature names for clarity
feature_names = X_train_resampled.columns
selected_features = feature_names[selected_indices]

#Reduce datasets to selected features
X_train_selected = X_train_resampled.iloc[:, selected_indices]
X_test_selected = X_test.iloc[:, selected_indices]
#Reduces training and test datasets to the selected feature subset.


print(f"Selected {len(selected_indices)} features from original {X_train_resampled.shape[1]}")
print("Selected features:")
print(selected_features)
#Prints out how many features were retained and their names.

#eigenvalues(explained variance) , eigenvectors(component direction)

# Final Upsampling Datasets with Selected Features and Correct Column Names

df_1_train_up = pd.DataFrame(X_train_selected, columns=X_train_selected.columns)
df_1_train_up['bankrupt'] = y_train_resampled

df_1_test_up = pd.DataFrame(X_test_selected, columns=X_test_selected.columns)
df_1_test_up['bankrupt'] = y_test

df_1_train_up.shape

#Downsampling (removed)











## SVM ##

#Upsampling Data Name

# Training set
X_train_up = df_1_train_up.drop('bankrupt', axis=1)
y_train_up = df_1_train_up['bankrupt']

# Testing set
X_test_up = df_1_test_up.drop('bankrupt', axis=1)
y_test_up = df_1_test_up['bankrupt']

y_test_up.shape

#SVM , Up
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
#svm
svm = SVC(kernel='rbf',C=1,gamma='scale',random_state = 42)
svm.fit(X_train_up,y_train_up)
y_pred_up = svm.predict(X_test_up)

#confusion matrix
sns.heatmap(confusion_matrix(y_test_up,y_pred_up),annot = True, fmt = 'd',cmap ='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix Upsampling with SVM")
plt.show()

#SVM(Down)
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN
#SMOTE #Down-sampling
smoteenn = SMOTEENN(random_state=42)
X_train_down , y_train_down = smoteenn.fit_resample(X_train,y_train)
print(pd.Series(y_train_down).value_counts())

#PCA selection
pca = PCA(n_components = 0.95)
X_train_down_pca = pca.fit_transform(X_train_down)
X_test_down_pca = pca.transform(X_test)

#SVM , down
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
#svm
svm = SVC(kernel='rbf',C=1,gamma='scale',random_state = 42)
svm.fit(X_train_down_pca,y_train_down)


y_pred_down = svm.predict(X_test_down_pca)

#confusion matrix
sns.heatmap(confusion_matrix(y_test_up,y_pred_down),annot = True, fmt = 'd',cmap ='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix Downsampling with SVM")
plt.show()





















